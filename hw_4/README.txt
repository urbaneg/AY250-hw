In the ipython notebook 'hw_4.ipynb', I run a simulation that calculates pi via Monte Carlo. By "throwing darts" via a random number generator, we calculate the ratio the area of a circle of a certain diameter to the area of a square of the same width and from that extract pi. 

This simulation is run in 3 ways on a Intel(R) Core(TM) i7-3615QM CPU @ 2.30GHz laptop.

The first is a simple series Monte Carlo where each iteration is run, the result recorded, and the next "dart thrown". As you would expect, as the number of darts is increased, the duration of the simulation increases linearly as the function needs to evaluate more times.  In the plot at the end of the notebook, you can see that this is the case. The run time is linear with the number of darts and the simulation rate remains constant.

Next I use dask arrays to run the computation. This starts off the calulation if a dart is a hit or not but does not wait for the answer. Instead, recieves a pointer to the future answer and after starting all the calculations it goes back and looks up what the answer is. At the end of python notebook, you can see that Dask initially runs slower than the simple method.  This is because in addition to running the same calculation as the simple method, the module also has significant overhead from creating the dask arrays, storing the answer points, and fetching the answers. Based on my simulation, this all takes about 10ms. Therefore, if the number of darts is small, this overhead will actually give a constant slow down to the simulation. However, once the number of darts is large such that the calculation of interest takes longer than the overhead, we begin to see the benifits of delaying our answer retreval. The run time switchs from a constant time to a time that is linear with the number of darts but an order of magnitude less than the simple method. From this point on (10^5 darts), the simulation rate is constant but significatly larger than the simple method.

Finally, I use multiprocessing to run the simulation. Here, multiple calculations are done concurrently to maximize the use of the computers processing power. Like dask, multiprocessing has some overhead that isn't present in the simple method which slows it down for dart numbers under about 10. However, after that point it approaches the simple model. However, around 10000 darts, there is a kink in the simulation rate. This occurs becuase I am chunnking the process into groups of 10000. That means that until we hit that number, we don't achieve the benifit of multiprocessing. However, after that point, we see the simulation rate tick up again until it finally becomes constant.  Around 100000 darts we are getting the full benifit of the multithreading which is about a factor of 3 speed up over the simple simulation as we run multiple calculations (in batchs of 10000) at the same time.